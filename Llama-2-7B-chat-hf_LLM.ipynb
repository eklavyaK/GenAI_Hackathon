{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fcbf7c-252d-42a2-a738-6658fd9c360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from io import BytesIO\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import huggingface_pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Image as IPImage\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from diffusers import DPMSolverMultistepScheduler, StableDiffusionImg2ImgPipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569282a0-b460-4793-9b10-72e3195b4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "base = \"./base\"\n",
    "model_dir = \"/home/common/data/Big_Data/GenAI\"\n",
    "model_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "hf_aut = \"\" # put your hugging face authentication token\n",
    "scene = {\"city\" : 1, \"town\" : 1, \"urban\" : 1, \"village\" : 3, \"rural\" : 3, \n",
    "         \"forest\" : 5, \"jungle\" : 5, \"mountain\" : 7, \"hill\" : 7, \"sea\" : 9, \"ocean\" : 9, \"aqua\" : 9, \n",
    "         \"room\" : 11, \"house\" : 11, \"space\" : 13, \"universe\" : 13, \"cosmos\" : 13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3523030-8a63-4931-9fae-462f29a75c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Template(instruction, new_system_prompt = DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return template\n",
    "\n",
    "def split_paragraphs(story_text, max_words_per_paragraph = 150):\n",
    "    if story_text is None:\n",
    "        story_text = \"a man\"\n",
    "        \n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', story_text.strip())\n",
    "\n",
    "    current_paragraph = ''\n",
    "    paragraphs = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_paragraph += sentence.strip() + ' '\n",
    "\n",
    "        if len(current_paragraph.split()) > max_words_per_paragraph:\n",
    "            sentences_in_paragraph = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', current_paragraph.strip())\n",
    "\n",
    "            current_paragraph = ' '.join(sentences_in_paragraph[:-1]).strip()\n",
    "\n",
    "            paragraphs.append(current_paragraph)\n",
    "\n",
    "            current_paragraph = sentence.strip() + ' '\n",
    "\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(current_paragraph)\n",
    "\n",
    "    return paragraphs\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "#                                           token = hf_aut,)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     device_map = 'xpu',\n",
    "#     torch_dtype = torch.float32,\n",
    "#     token = hf_aut,\n",
    "#     )\n",
    "\n",
    "# pipe = transformers.pipeline(\n",
    "#     task = \"text-generation\",\n",
    "#     model = model,\n",
    "#     device_map = 'xpu',\n",
    "#     tokenizer = tokenizer,\n",
    "#     return_full_text = True,\n",
    "#     max_new_tokens = 3000,\n",
    "#     do_sample = True,\n",
    "#     top_k = 50,\n",
    "#     num_return_sequences = 1,\n",
    "#     eos_token_id = tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c46713-523a-438c-b58d-f13d3a1d5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You write long, wonderful and creative stories on provided topic. \\\n",
    "A short description of the story will be provided to you. \\\n",
    "You have to generate a good story which should be based on the provided description\"\n",
    "instruction = \"Write a story on the following topic in 2500 words:\\n\\n {text}\"\n",
    "template_story = Template(instruction, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2f4aac-5c23-4a42-804a-39edbc55f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You're an expert prompt generator. The prompt generated by you will be fed to a image generation model.  A part of a story will be provided to you and you have to generate a simple prompt that describes the scenario in that part of the story such that the part of the story can be explained in an image generated by the prompt generated by you.\n",
    "    \n",
    "    Here are some rules u have to follow while generating the prompts: \n",
    "    1. The prompt be strictly less than 70 words.\n",
    "    2. Don't include special characters other than comma and hyphen and dot.\n",
    "    3. You just have to describe the scenario not write the whole story.\n",
    "    4. Always include \"A colored cartoon type sketch of,\" at the start of every prompt.\n",
    "    5. The very important one, write in crisp and very simple english, don't use complicated words.\n",
    "    6. Separate the different traits of the scenario with commas.\n",
    "    7. If you can't understand the story or text, just write whatever you think the situation could be in the text.\n",
    "    \n",
    "    Here are some examples on how to generate the prompt:\n",
    "    \n",
    "    Example of a story paragraph:\n",
    "    Once upon a time, in a not-so-distant future, there lived a man named Alex. Alex was an adventurous soul who dreamed of exploring the great \\\n",
    "    unknown: outer space. From a young age, he would gaze up at the stars with wonder, imagining what it would be like to journey among them.\n",
    "    \n",
    "    Expected text from you is:\n",
    "    A colored cartoon type sketch of, a man looking up in the sky at night, sky has stars & moon.\n",
    "    \n",
    "    Example story paragraph:\n",
    "    As the days turned into weeks, Maya forged friendships with the creatures of the jungle. She shared moments of laughter with mischievous monkeys, \\\n",
    "    and learned the ancient wisdom of wise old elephants. Together, they explored hidden caves and winding rivers, each new discovery fueling Maya's sense of wonder.\n",
    "\n",
    "    Expected prompt from you is:\n",
    "    A colored cartoon type sketch of, A girl laughing with monkeys, old elephants, hidden caves, winding rivers.\n",
    "    \n",
    "    Example of another story paragraph:\n",
    "    In the heart of a bustling metropolis, where skyscrapers kissed the sky and streets hummed with the \\\n",
    "    rhythm of life, there existed a city like no other. Its streets were a labyrinth of winding alleys and bustling boulevards, \\\n",
    "    lined with towering buildings that reached for the clouds.\n",
    "\n",
    "    Expected prompt generated from you is:\n",
    "    A colored cartoon type sketch of, a metropolitan city, high skycrapers, streets, sky with clouds.\n",
    "    \n",
    "    Example story paragraph:\n",
    "    what's up\n",
    "\n",
    "    Since the paragraph is vague to understand, you can assume that a person is saying what's up to another person, for this the expected \\\n",
    "    prompt generated by you is:\n",
    "    A colored cartoon type sketch of, two person speaking.\n",
    "    \n",
    "    Further rules:\n",
    "\n",
    "    Please don't generate more than 70 words, this is a must.\n",
    "    Please note that all the above examples the generated prompts were less than 20 words, you must also generate the prompts strictly less than 70 words.\n",
    "    \n",
    "    I just want the prompt from you not the explanation of why you generated that prompt.\n",
    "    \"\"\"\n",
    "\n",
    "instruction = \"Generate a prompt less than 70 words for the story paragraph:\\n\\n {text}\"\n",
    "template_prompt = Template(instruction, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210fd4db-960f-4b4e-8cea-2a01a401e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You're an expert background scenario recognizer. You will be given a scene of a story you have to read that scene and try to recognize the most suitable background or surrounding scenario where the scene is taking place.\n",
    "    \n",
    "    You have to give the output ONLY from below options provided.\n",
    "    \n",
    "    1. city\n",
    "    2. village\n",
    "    3. forest\n",
    "    4. mountain\n",
    "    5. sea\n",
    "    7. room\n",
    "    6. space\n",
    "\n",
    "    You should output the backgrounds from these six options only. You must not output any other background which is not listed above.\n",
    "    If you can't understand the background in the story just output the background as village by default.\n",
    "\n",
    "    Here are some examples on how to generate the required option:\n",
    "    \n",
    "    Example story paragraph:\n",
    "    In the heart of a bustling metropolis, where skyscrapers kissed the sky and streets hummed with the \\\n",
    "    rhythm of life, there existed a city like no other. Its streets were a labyrinth of winding alleys and bustling boulevards, \\\n",
    "    lined with towering buildings that reached for the clouds.\n",
    "    \n",
    "    Expected guess:\n",
    "    As from the above paragraph we can guess that the surrounding is of city because metropolis and skycrapers are mentioned. \\\n",
    "    Hence the text generated by you is:\n",
    "    From the story I infer the background scene as city\n",
    "\n",
    "    Example story paragraph:\n",
    "    As the days turned into weeks, Maya forged friendships with the creatures of the jungle. She shared moments of laughter with mischievous monkeys, \\\n",
    "    and learned the ancient wisdom of wise old elephants. Together, they explored hidden caves and winding rivers, each new discovery fueling Maya's sense of wonder.\n",
    "    \n",
    "    Expected guess:\n",
    "    As from the above paragraph we can guess that the surrounding is of forest / jungle because of monkeys, rivers and caves. \\\n",
    "    Hence the text generated by you is:\n",
    "    From the story I infer the background scene as forest\n",
    "\n",
    "    Example story paragraph:\n",
    "    Hey man how are you? All good!\n",
    "\n",
    "    Expected guess:\n",
    "    Since the location / surrounding can't be guessed because there is no hint present in the paragraph you have to provide the default option which is village. \\\n",
    "    Hence the text generated by you is:\n",
    "    From the story I infer the background scene as village\n",
    "    \"\"\"\n",
    "\n",
    "instruction = \"Guess the background scene for the story paragraph:\\n\\n {text}\"\n",
    "template_scene = Template(instruction, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b530da-cec2-4936-a1f7-ff6da00d949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You're an expert at guessing whether a event is occuring in day or night based on the description of the event.\\\n",
    "You will be given a scene of a story which you have to read. After reading the scene you have to tell whether the scene in the story is taking place in day or in night.\n",
    "In case you're not able to guess between night or day, then by default output day.\n",
    "\n",
    "    You must output only one of the following options:\n",
    "    1. day\n",
    "    2. night\n",
    "\n",
    "    You should not output times like: afternoon, morning, evening etc. You have to output only either day or night.\n",
    "    In case you're not able to guess between night or day, then by default output day.\n",
    "\n",
    "    Here are some examples on how to generate the required option:\n",
    "    \n",
    "    Example story paragraph:\n",
    "    Once upon a sun-kissed morning, in the heart of a serene village nestled amidst rolling hills and lush greenery, a bustling day began to unfold. \\\n",
    "    The village, with its quaint cottages and winding pathways, exuded an aura of tranquility under the clear blue sky.\n",
    "    \n",
    "    Expected guess: day\n",
    "\n",
    "    Example story paragraph:\n",
    "    In the afternoon, as the village stirred with activity, Sarah joined her neighbors in the bustling marketplace.\\\n",
    "    Amidst stalls laden with fresh produce and the lively chatter of vendors and customers alike, she exchanged greetings and stories with familiar faces,\\\n",
    "    weaving the fabric of community that bound them together.\n",
    "    \n",
    "    Expected guess: day\n",
    "\n",
    "    Example story paragraph:\n",
    "    Under the velvet embrace of the starlit night sky, a mysterious tale unfolds in the shadowed corners of a forgotten town. \\\n",
    "    The moon, a solitary sentinel, casts its silvery glow upon the cobblestone streets, illuminating secrets hidden in the darkness.\n",
    "\n",
    "    Expected guess: night\n",
    "\n",
    "    Example story paragraph:\n",
    "    As the sun dipped below the horizon, painting the sky with hues of crimson and gold, the sleepy town of Willowbrook stirred to life once more. \\\n",
    "    In the tranquil streets lined with quaint cottages and flickering lanterns, a tale of love and longing began to unfold.\n",
    "\n",
    "    Expected guess: night\n",
    "\n",
    "    Example story paragraph:\n",
    "    Hey man! what's up.\n",
    "\n",
    "    Expected guess: Since, we're not able to guess anything, then by default you should output day\n",
    "    \"\"\"\n",
    "\n",
    "instruction = \"Guess the background scene for the story paragraph:\\n\\n {text}\"\n",
    "template_time = Template(instruction, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fad55f-af8f-4e4c-959c-3c56567a0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path = model_path,\n",
    "        hf_aut = hf_aut,\n",
    "        torch_dtype = torch.float16,\n",
    "        top_k = 50,\n",
    "        max_tokens = 3000,\n",
    "        device_map = 'xpu',\n",
    "        temperature = 0.1,\n",
    "        optimize = True,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.device_map = device_map\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.hf_aut = hf_aut\n",
    "        self.model_path = model_path\n",
    "        self.generator = torch.Generator()\n",
    "        self.pipeline = self._load_pipeline(model_path = model_path, \n",
    "                                            torch_dtype = torch_dtype, \n",
    "                                            hf_aut = hf_aut, \n",
    "                                            device_map = device_map, \n",
    "                                            top_k = top_k,\n",
    "                                            max_tokens = max_tokens)\n",
    "  \n",
    "        self.llm = huggingface_pipeline.HuggingFacePipeline(pipeline = self.pipeline, model_kwargs = {'temperature': temperature})\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self, model_path, torch_dtype, hf_aut, device_map, top_k, max_tokens\n",
    "    ):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          token = hf_aut,)\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map = 'xpu',\n",
    "            torch_dtype = torch_dtype,\n",
    "            token = hf_aut,\n",
    "            )\n",
    "        \n",
    "        pipe = transformers.pipeline(\n",
    "            task = \"text-generation\",\n",
    "            model = model,\n",
    "            device_map = device_map,\n",
    "            tokenizer = tokenizer,\n",
    "            return_full_text = True,\n",
    "            max_new_tokens = max_tokens,\n",
    "            do_sample = True,\n",
    "            top_k = top_k,\n",
    "            num_return_sequences = 1,\n",
    "            eos_token_id = tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        return pipe\n",
    "      \n",
    "    def clean_output(self, text):\n",
    "        text = text['text']\n",
    "        split = text.split(E_INST)\n",
    "        if len(split) == 0: return \" \"\n",
    "        elif len(split) == 1: return split[0]\n",
    "        else:\n",
    "            split = split[1:]\n",
    "            cur = \"\"\n",
    "            for i in split: cur += i\n",
    "            return cur\n",
    "            \n",
    "    def generate(self, template, text):\n",
    "        prompt = PromptTemplate(template = template, input_variables = [\"text\"])\n",
    "        model = LLMChain(prompt = prompt, llm = self.llm)\n",
    "        return self.clean_output(model.invoke(text))\n",
    "    \n",
    "    def check_time(self, text):\n",
    "        if \"day\" in text: return 0\n",
    "        return 1\n",
    "    \n",
    "    def check_scene(self, text):\n",
    "        for key, value in scene.items():\n",
    "            if key in text: return value\n",
    "        return 3\n",
    "    \n",
    "    def get_base(self, text):\n",
    "        output = self.generate(template_scene, text)\n",
    "        output = output.lower()\n",
    "        sc = self.check_scene(output)\n",
    "        if sc == 13: return sc\n",
    "        output = self.generate(template_time, text)\n",
    "        return sc + self.check_time(output)\n",
    "\n",
    "    def get_story(self, text):\n",
    "        return self.generate(template_story, text)\n",
    "\n",
    "    def get_prompt(self, text):\n",
    "        return self.generate(template_prompt, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492177d2-2a3e-4f59-9c8b-655755a255f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da111d50522c43e481e20244f38b8f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "llama = LLM(model_path, hf_aut, top_k = 50, max_tokens = 3000, temperature = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856c72bc-bec4-4571-8715-21089f3601d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img2ImgModel:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        device: str = \"xpu\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "        warmup: bool = False,\n",
    "        scheduler: bool = True,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.device = device\n",
    "        self.data_type = torch_dtype\n",
    "        self.scheduler = scheduler\n",
    "        self.generator = torch.Generator()\n",
    "        self.pipeline = self._load_pipeline(model_id_or_path, torch_dtype)\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self, model_id_or_path: str, torch_dtype: torch.dtype\n",
    "    ) -> StableDiffusionImg2ImgPipeline:\n",
    "        \n",
    "        model_path = Path(f\"{model_dir}/{model_id_or_path}\")\n",
    "        \n",
    "        if model_path.exists():\n",
    "            load_path = model_path\n",
    "        else:\n",
    "            print(\"Using the default path for models...\")\n",
    "            load_path = model_id_or_path\n",
    "            \n",
    "        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            load_path,\n",
    "            torch_dtype = torch_dtype,\n",
    "            use_safetensors = True,\n",
    "            variant = \"fp16\",\n",
    "        )\n",
    "        if self.scheduler:\n",
    "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                pipeline.scheduler.config\n",
    "            )\n",
    "        if not model_path.exists():\n",
    "            try:\n",
    "                print(f\"Attempting to save the model to {model_path}...\")\n",
    "                pipeline.save_pretrained(f\"{model_path}\")\n",
    "                print(\"Model saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while saving the model: {e}. Proceeding without saving.\")\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        return pipeline\n",
    "\n",
    "    def get_image(self, prompt) -> Image.Image:\n",
    "        image_number = 3\n",
    "        try:\n",
    "            image_number = llama.get_base(prompt)\n",
    "        except:\n",
    "            image_number = 3\n",
    "        if image_number < 1 or image_number > 13:\n",
    "            image_number = 3\n",
    "        img = Image.open(f'{base}/{image_number}'+'.jpg')\n",
    "        img = img.resize((800, 500))\n",
    "        return img\n",
    "\n",
    "    def generate_images(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        num_inference_steps: int = 200,\n",
    "        strength: float = 0.75,\n",
    "        guidance_scale: float = 7.5,\n",
    "        batch_size: int = 1,\n",
    "    ):\n",
    "        init_image = self.get_image(prompt)\n",
    "        try:\n",
    "            prompt = llama.get_prompt(prompt)\n",
    "        except:\n",
    "            prompt = prompt\n",
    "\n",
    "        try:\n",
    "            image = self.pipeline(\n",
    "                prompt = prompt,\n",
    "                image = init_image,\n",
    "                strength = strength,\n",
    "                guidance_scale = guidance_scale,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "            ).images\n",
    "        \n",
    "            return image[0]\n",
    "\n",
    "        except:\n",
    "            return init_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa128428-b5f7-43c5-bb25-32755d24b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache = {}\n",
    "\n",
    "def generate_story():\n",
    "    out = widgets.Output()\n",
    "    model_ids = [\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"stabilityai/stable-diffusion-2-1\",\n",
    "    ]\n",
    "    model_dropdown = widgets.Dropdown(options = model_ids, value = model_ids[0], description = \"Select Model:\",) \n",
    "    prompt_text = widgets.Text(value=\"\", placeholder = \"Describe your story in short\", description = \"Enter Story:\", layout = widgets.Layout(width = \"600px\"))\n",
    "    \n",
    "    layout = widgets.Layout(margin = \"10px\")\n",
    "    button1 = widgets.Button(description = \"Generate Story\", button_style = \"primary\")\n",
    "    button2 = widgets.Button(description = \"Clear Story\", button_style = \"primary\")\n",
    "    model_dropdown.layout.width = \"50%\"\n",
    "    prompt_text.layout.width = \"600px\"\n",
    "    button1.layout.margin = \"0 0 0 100px\"\n",
    "    button1.layout.width = \"150px\"\n",
    "    button2.layout.margin = \"0 0 0 300px\"\n",
    "    button2.layout.width = \"120px\"\n",
    "    top_row = widgets.HBox([model_dropdown])\n",
    "    bottom_row = widgets.HBox([prompt_text])\n",
    "    top_box = widgets.VBox([top_row, bottom_row])\n",
    "    user_input_widgets = widgets.HBox([top_box], layout = layout)\n",
    "    bottom_box = widgets.HBox([button1, button2], layout = layout)\n",
    "    display(user_input_widgets)\n",
    "    display(bottom_box)\n",
    "    display(out)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def generate_image(button):\n",
    "        clear_output(wait = True)\n",
    "        print(\"Creating a new story...\")\n",
    "        story = llama.get_story(prompt_text.value)\n",
    "        partial_stories = split_paragraphs(story)\n",
    "        for i, parts in enumerate(partial_stories):\n",
    "            with out:\n",
    "                button.button_style = \"warning\"\n",
    "                selected_model_index = model_ids.index(model_dropdown.value)\n",
    "                model_id = model_ids[selected_model_index]\n",
    "                model_key = (model_id, \"xpu\")\n",
    "                if model_key not in model_cache:\n",
    "                    model_cache[model_key] = Img2ImgModel(model_id, device = \"xpu\")\n",
    "                prompt = parts\n",
    "                model = model_cache[model_key]\n",
    "                if not prompt:\n",
    "                    prompt = \"a village man\"  \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    image = model.generate_images(prompt = prompt,)\n",
    "                    print(prompt)\n",
    "                    image.show()\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"\\nUser interrupted image generation...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                finally:\n",
    "                    button.button_style = \"primary\"\n",
    "\n",
    "    def end_story(button):\n",
    "        with out:\n",
    "            clear_output(wait = True)\n",
    "            print(\"Creating a new story....\")\n",
    "            \n",
    "    button1.on_click(generate_image)\n",
    "    button2.on_click(end_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a08ecd44-4105-4c32-ab8a-aa33ac24a667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a2477596274824adf5b10787507060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Dropdown(description='Select Model:', layout=Layout(width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc796985c4df4a76b28d31ea53b3ff69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='primary', description='Generate Story', layout=Layout(margin='0 0 0 100px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab47e3a23f994b2d9b0f9a3c4971171e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_story()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e31d-b988-477e-8b23-81157c86976b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c94f93-8d74-43a1-b885-ea9cd80ff1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
